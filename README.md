# <p align=center>`RGBD-pretraining in DFormer`</p>

> **Authors:**
> [Bowen Yin](https://scholar.google.com/citations?user=xr_FRrEAAAAJ&hl=zh-CN&oi=sra),
> [Xuying Zhang](https://scholar.google.com/citations?hl=zh-CN&user=huWpVyEAAAAJ),
> [Zhongyu Li](https://scholar.google.com/citations?user=g6WHXrgAAAAJ&hl=zh-CN),
> [Li Liu](https://scholar.google.com/citations?hl=zh-CN&user=9cMQrVsAAAAJ),
> [Ming-Ming Cheng](https://scholar.google.com/citations?hl=zh-CN&user=huWpVyEAAAAJ),
> [Qibin Hou*](https://scholar.google.com/citations?user=fF8OFV8AAAAJ&hl=zh-CN)


This repository provides the RGBD pretraining code of '[ICLR 2024] DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation'.
Our implementation is modified from the [timm](https://github.com/huggingface/pytorch-image-models) repository.
If there are any questions, please let me know via [raising issues](https://github.com/VCIP-RGBD/DFormer/issues) or e-mail (bowenyin@mail.nankai.edu.cn).

## 1. ðŸš€ Get Start

**1.1. Install**

Enviroment requirement: Pyotrch & timm

If you have installed the dformer enviroment in our main repository, you can only additionally install timm.
```
conda create -n RGBD_Pretrain python=3.10 -y
conda activate RGBD_Pretrain
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113
pip install timm fvcore
```

If the above pipeline not work, You can also install following the [timm](https://github.com/huggingface/pytorch-image-models).


**1.2. Prepare Datasets**

First, you need to prepare the ImageNet-1k dataset.
We share the depth maps for the ImageNet-1K in the following links:


| [Baidu Netdisk](https://pan.baidu.com/s/1Ahzxzb1WL8fM0QSn0pbScw?pwd=7rz5) | [OneDrive(Coming Soon)]() |
|  ----  | ----  |


If the share links have any questions, please let me know (bowenyin@mail.nankai.edu.cn). 
Then, create the soft links:

```
ln -s path_to_imagenet datasets/ImageNet
ln -s path_to_imagenet_depth_maps datasets/Depth_ImageNet
```

## 2. ðŸš€ Train.

```
bash train.sh
```

After training, the checkpoints will be saved in the path `outputs/XXX', where the XXX is depends on the training config.

>Then, the pretrained checkpoint is endowed with the capacity to encode the RGBD represetions and can be applied to various RGBD tasks. 



> We invite all to contribute in making this project and RGBD representation learning more acessible and useful. If you have any questions or suggestions about our work, feel free to contact me via e-mail (bowenyin@mail.nankai.edu.cn) or raise an issue. 


## Reference
```
@article{yin2023dformer,
  title={DFormer: Rethinking RGBD Representation Learning for Semantic Segmentation},
  author={Yin, Bowen and Zhang, Xuying and Li, Zhongyu and Liu, Li and Cheng, Ming-Ming and Hou, Qibin},
  journal={arXiv preprint arXiv:2309.09668},
  year={2023}
}
```


### Acknowledgment

Our implementation is mainly based on [timm](https://github.com/huggingface/pytorch-image-models). The depth maps are generated by [Omnidata](https://github.com/EPFL-VILAB/omnidata). Thanks for their authors.

### License

Code in this repo is for non-commercial use only.